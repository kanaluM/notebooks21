{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jQ1tEQCxwRx"
   },
   "source": [
    "# Deep Convolutional Generative Adversarial Network\n",
    "\n",
    "Notebook adapted from [this site](https://www.tensorflow.org/tutorials/generative/dcgan)\n",
    "\n",
    "This tutorial demonstrates how to generate images of handwritten digits using a [Deep Convolutional Generative Adversarial Network](https://arxiv.org/pdf/1511.06434.pdf) (DCGAN). The code is written using the [Keras Sequential API](https://www.tensorflow.org/guide/keras) with a `tf.GradientTape` training loop.\n",
    "\n",
    "# What are GANs?\n",
    "\n",
    "[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A *generator* (\"the artist\") learns to create images that look real, while a *discriminator* (\"the art critic\") learns to tell real images apart from fakes.\n",
    "\n",
    "During training, the *generator* progressively becomes better at creating images that look real, while the *discriminator* becomes better at telling them apart. The process reaches equilibrium when the *discriminator* can no longer distinguish real images from fakes.\n",
    "\n",
    "This notebook demonstrates this process on the MNIST dataset.\n",
    "\n",
    "First download the [tensorflow library](https://www.tensorflow.org/) (if needed)  \n",
    "`pip3 install --user --upgrade tensorflow`\n",
    "\n",
    "Also run these commands to get the libraries needed to generate GIFs of your results  \n",
    "`!pip install imageio`  \n",
    "`!pip install git+https://github.com/tensorflow/docs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:10.883821Z",
     "iopub.status.busy": "2021-06-16T15:52:10.883225Z",
     "iopub.status.idle": "2021-06-16T15:52:11.123447Z",
     "shell.execute_reply": "2021-06-16T15:52:11.122880Z"
    },
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from IPython import display\n",
    "import tensorflow_docs.vis.embed as embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "# The _NEW_ Digits Dataset\n",
    "\n",
    "Hopefully by now you are familiar with the digits data set we have been using. These 8x8 images are part of scikit-learn's digits data set. However, you may have noticed that many of these images do not clearly resemble actual digits!\n",
    "\n",
    "Here we will use the MNIST dataset to train our GAN generator and discriminator. These digits are a bit larger - 28x28 - and will hopefully look more \"real\" than the digits we have been using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:11.127510Z",
     "iopub.status.busy": "2021-06-16T15:52:11.126953Z",
     "iopub.status.idle": "2021-06-16T15:52:11.395613Z",
     "shell.execute_reply": "2021-06-16T15:52:11.395077Z"
    },
    "id": "a4fYMGxGhrna"
   },
   "outputs": [],
   "source": [
    "# load the images into an array called \"trainImages\"\n",
    "# load the digit labels into an array called \"trainLabels\"\n",
    "(trainImages, trainLabels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "print(f\"There are {len(trainImages)} digit images and labels\")\n",
    "print(f\"Each image is {len(trainImages[0])} x {len(trainImages[0][0])} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use matplotlib's imshow function to display the data\n",
    "rando = np.random.randint(len(trainImages))\n",
    "print(f\"Showing the {rando}th digit ({trainLabels[rando]})\")\n",
    "fig = plt.figure()\n",
    "plt.imshow(trainImages[rando], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting and Cleaning the Data\n",
    "\n",
    "First, notice that this data set contains 60,000 digits! This is much more than we need, so we will use a 1000-image subset to train the GAN.\n",
    "\n",
    "To do this, we will use the provided `sortByNumber(digit, numSamples)` function. This function will return a subset of `numSamples` images of a single `digit` (0-9). Later, you may consider training your GAN on _all_ of the digits, but for now we will simply train our GAN to create the digit '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's filter out training data\n",
    "def sortByNumber(digit, numSamples):\n",
    "    \"\"\"Returns an array of numSamples images of a digit\n",
    "    Use digit = -1 to include all digits 0-9 inclusive\n",
    "    Use numSamples = -1 for maximum value of numSamples\"\"\"\n",
    "    \n",
    "    numberImages = []\n",
    "    if digit == -1:\n",
    "        sortedData = trainImages\n",
    "    else:\n",
    "        for i in range(60000):\n",
    "            if trainLabels[i] == digit:\n",
    "                numberImages += [trainImages[i]]\n",
    "        sortedData = np.asarray(numberImages)\n",
    "        \n",
    "    np.random.shuffle(sortedData)\n",
    "    \n",
    "    if 0 < numSamples < len(sortedData):\n",
    "        sortedData = sortedData[:numSamples]\n",
    "        \n",
    "    return sortedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGIT = 2\n",
    "NUM_SAMPLES = 1000\n",
    "sortedImages = sortByNumber(DIGIT, NUM_SAMPLES)\n",
    "print(f\"There are {len(sortedImages)} training images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, ``sortedImages`` is a 3D array: each image is a 2D array and we have a whole list of images. However, the CNN functions used in TensorFlow take in 4D arrays as inputs! Therefore we will have to reshape the data by putting each single grayscale value into its own nested list.\n",
    "\n",
    "In general, the 4D arrays are of the form (batchSize, height, width, channelSize)\n",
    "\n",
    "<img src=\"input.png\" width=300 />\n",
    "\n",
    "- batchSize: the number of training inputs that are used each time the generator and discriminator are updated. For this example we will separate our 1000 images into batches of 50\n",
    "- height: the height of the image. The training imgages have height 28\n",
    "- width: the width of the image output. The training imgages have width 28\n",
    "- channelSize: the complexity of the pixel data. For example, RGB pixels have channelSize 3. Since we are in grayscale, channelSize is simply 1\n",
    "\n",
    "The current pixel data consists of integers on [0,255]. However, it will be better if we transform the pixel data to floating point numbers on [-1,1].\n",
    "\n",
    "For the curious, ``BUFFER_SIZE`` is related to how we randomly order the training data. Using a buffer size equal to the number of training images is generally desirable because every image has an equal chance of being picked next in the final training data set. Check out [the docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) to learn more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = NUM_SAMPLES  # use perfectly random sampling among all 1000 digit images\n",
    "BATCH_SIZE = 50   # train model on 50 images per loop\n",
    "\n",
    "# create the 4D array!\n",
    "cleanImageData = sortedImages.reshape(sortedImages.shape[0], 28, 28, 1).astype('float32')   # 3D -> 4D\n",
    "cleanImageData = (cleanImageData - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "# Batch and shuffle the data\n",
    "trainingData = tf.data.Dataset.from_tensor_slices(cleanImageData).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "# The Big Picture\n",
    "\n",
    "Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model). The generator takes a random noise vector and turns it into an image. The discriminator takes an image input and turns it into a single number. Both accomplish this transformation by iteratively transforming their input's shape very slightly; these small transformations are called _layers_. They are trained simultaneously until the discriminator can no longer distinguish between real digits (training data) and fake digits (from the generator).\n",
    "\n",
    "<img src=\"gist.png\" width=300 />\n",
    "\n",
    "\n",
    "# The Generator\n",
    "\n",
    "The generator uses `Dense` and `Conv2DTranspose` layers to produce an image from a seed (a list of random numbers). The generator takes this list as input, then _upsamples_ several times until it reaches the desired image size of 28x28x1. _Upsampling_ is the process of constructing a larger image from a smaller image. More details can be found [here](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d).\n",
    "\n",
    "<img src=\"generator.png\" width=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:12.954211Z",
     "iopub.status.busy": "2021-06-16T15:52:12.953611Z",
     "iopub.status.idle": "2021-06-16T15:52:12.955867Z",
     "shell.execute_reply": "2021-06-16T15:52:12.955433Z"
    },
    "id": "6bpTcDqoLWjY"
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 100\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # add a Dense layer first\n",
    "    model.add(layers.Dense(7*7*64, use_bias=False, input_shape=(INPUT_SIZE,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 64)))\n",
    "    assert model.output_shape == (None, 7, 7, 64)  # None is the batch size\n",
    "\n",
    "    # add a Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(1, 1), \n",
    "        padding='same', \n",
    "        use_bias=False))\n",
    "    \n",
    "    assert model.output_shape == (None, 7, 7, 32)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    # add another Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(\n",
    "        filters=8, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(2, 2), \n",
    "        padding='same', \n",
    "        use_bias=False))\n",
    "\n",
    "    assert model.output_shape == (None, 14, 14, 8)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # add another Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(\n",
    "        filters=1, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(2, 2), \n",
    "        padding='same', \n",
    "        use_bias=False,\n",
    "        activation='tanh'))\n",
    "    \n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWgG09LCSJl"
   },
   "source": [
    "# Understanding the Generator \n",
    "\n",
    "Let's take a closer look at the generator. Run the cell below to see some of the inner workings of it and its layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:12.959972Z",
     "iopub.status.busy": "2021-06-16T15:52:12.959379Z",
     "iopub.status.idle": "2021-06-16T15:52:14.853260Z",
     "shell.execute_reply": "2021-06-16T15:52:14.853656Z"
    },
    "id": "gl7jcC7TdPTG"
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the argument `None` of the layers' input vectors refers to the batch size and will be filled in with the appropriate value when we actually start using the generator\n",
    "\n",
    "Notice that our random noise input lists (length `INPUT_SIZE` = 100) are first transformed into 3136 element lists. These are then transformed into 7x7x64 images (7x7 pixels with 64 element lists for each pixel's data?!). Eventually these are transformed into the desired 28x28x1 images\n",
    "\n",
    "# Testing the Generator\n",
    "\n",
    "Let's test the generator's initial capabilities \n",
    "\n",
    "First we create a \"noise\" vector. This is a list floating point numbers (length = `INPUT_SIZE`). These values are randomly selected from a normal distribution (mean 0 and standard deviation 1). This acts as the input to our generator model. The generator takes in this 1D array input and does some fancy math to turn it into our desired 4D array output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, INPUT_SIZE])   # a list of random floating point numbers\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0IKnaCtg6WE"
   },
   "source": [
    "You might notice that the picture generated below does not resemble a digit. This is because the generator has not been trained. However, we have successfully transformed a 100 element 1D array into a 28x28 image!\n",
    "\n",
    "# The Discriminator\n",
    "\n",
    "The discriminator is a convolutional neural network (CNN) based image classifier. The discriminator works in the opposite direction of the generator. It uses `Conv2D` layers to take an image and transform it down into a single number output. The discriminator will be trained to output positive numbers for real images and negative numbers for fake images.\n",
    "\n",
    "<img src=\"layers.png\" width=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.859368Z",
     "iopub.status.busy": "2021-06-16T15:52:14.858794Z",
     "iopub.status.idle": "2021-06-16T15:52:14.860978Z",
     "shell.execute_reply": "2021-06-16T15:52:14.860486Z"
    },
    "id": "dw2tPLmk2pEP"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(2, 2), \n",
    "        padding='same',\n",
    "        input_shape=[28, 28, 1]))\n",
    "    \n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=64, \n",
    "        kernel_size=(3, 3), \n",
    "        strides=(2, 2), \n",
    "        padding='same'))\n",
    "    \n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhPneagzCaQv"
   },
   "source": [
    "# Understanding the Discriminator\n",
    "\n",
    "Let's take a closer look at the discriminator. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.865139Z",
     "iopub.status.busy": "2021-06-16T15:52:14.864594Z",
     "iopub.status.idle": "2021-06-16T15:52:14.909629Z",
     "shell.execute_reply": "2021-06-16T15:52:14.909971Z"
    },
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the argument `None` of the layers' input vectors refers to the batch size and will be filled in with the appropriate value when we actually start using the discriminator\n",
    "\n",
    "Notice that our images are first reshaped into 14x14x32 images (14x14 pixels with 32 element lists for each pixel's data). Eventually these are transformed into a single value\n",
    "\n",
    "# Testing the Discriminator\n",
    "\n",
    "Let's test the discriminator's initial capabilities \n",
    "\n",
    "We will pass in the `generated_image` from the untrained generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(generated_image, training=False)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "You might notice that the discriminator returns a value very close to 0. This is because the discriminator is also untrained and has no way to distinguish between real (positive output) and fake (negative output). However, we have successfully transformed a 28x28 image into a single number!\n",
    "\n",
    "# Define the loss and optimizers\n",
    "\n",
    "We can now define the functions that mathematically calculate how well the GAN is being trained. The below `cross_entropy` is a function that will calculate a measure of how different two distributions are. It will use the [logit](https://en.wikipedia.org/wiki/Logit) function to convert the positive/negative output from the discriminator to a probability from 0 to 1 (where 0 indicates a fake image and 1 represents a real image). Read more [here](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.913440Z",
     "iopub.status.busy": "2021-06-16T15:52:14.912839Z",
     "iopub.status.idle": "2021-06-16T15:52:14.915280Z",
     "shell.execute_reply": "2021-06-16T15:52:14.914746Z"
    },
    "id": "psQfmXxYKU3X"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKY_iPSPNWoj"
   },
   "source": [
    "### Discriminator loss\n",
    "\n",
    "This function quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s (because 1 corresponds to real images), and the discriminator's predictions on fake (generated) images to an array of 0s (because 0 corresponds to fake images). Ideally this `discriminator_loss` value will decrease as training occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.919321Z",
     "iopub.status.busy": "2021-06-16T15:52:14.918775Z",
     "iopub.status.idle": "2021-06-16T15:52:14.920995Z",
     "shell.execute_reply": "2021-06-16T15:52:14.920457Z"
    },
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd-3GCUEiKtv"
   },
   "source": [
    "### Generator loss\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, compare the discriminator's decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.924650Z",
     "iopub.status.busy": "2021-06-16T15:52:14.924084Z",
     "iopub.status.idle": "2021-06-16T15:52:14.925998Z",
     "shell.execute_reply": "2021-06-16T15:52:14.925512Z"
    },
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgIc7i0th_Iu"
   },
   "source": [
    "### Adam\n",
    "\n",
    "The Adam algorithm is simply a method of adjusting the model after training. Recall that neural nets are multivariable functions, and we can optimize such functions using the gradient. The Adam algorithm is a method of actually traversing the gradient. It will nudge the generator and discriminator in the right direction as training occurs.\n",
    "\n",
    "The discriminator and the generator optimizers are different since you will train two networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.929800Z",
     "iopub.status.busy": "2021-06-16T15:52:14.929263Z",
     "iopub.status.idle": "2021-06-16T15:52:14.931092Z",
     "shell.execute_reply": "2021-06-16T15:52:14.930605Z"
    },
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "# Define the training loop\n",
    "\n",
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator.\n",
    "\n",
    "In fact, we can crudely study the GAN's progress by collecting data on the discriminator's performance. We will select a random training image and keep track of the discriminator's judgement on it as it gets trained. We will also select a random input to get passed through the generator and keep track of the discriminator's judgement on this fake image. Hopefully the discriminator's judgement on these two images will converge as the GAN is trained.\n",
    "\n",
    "The `train_step` is the most fundamental function of the training loop and actually executed the process described above. Notice the use of `@tf.function`. This causes the code to switch from _eager_ execution to _graph_ execution. In short, this makes things run faster! If interested, more details can be found [here](https://towardsdatascience.com/eager-execution-vs-graph-execution-which-is-better-38162ea4dbf6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random generator input for data collection\n",
    "testInput = tf.random.normal([1, INPUT_SIZE])\n",
    "\n",
    "# choose a random real image for data collection\n",
    "testImage = cleanImageData[np.random.randint(len(cleanImageData))].reshape(1, 28, 28, 1)\n",
    "\n",
    "fakeError = []   # discriminator's judgement on the generator's fake image\n",
    "realError = []   # discriminator's judgement on the real training image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.947707Z",
     "iopub.status.busy": "2021-06-16T15:52:14.947128Z",
     "iopub.status.idle": "2021-06-16T15:52:14.948632Z",
     "shell.execute_reply": "2021-06-16T15:52:14.948967Z"
    },
    "id": "3t5ibNo05jCB"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, INPUT_SIZE])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aFF7Hk3XdeW"
   },
   "source": [
    "### Generate and save images\n",
    "\n",
    "To visulize the GAN's progress, we will observe how 16 different random inputs are transformed by the generator as training occurs. Use the line below to create a folder to store these images.\n",
    "\n",
    "The `generate_and_save_images` helper function will display and save the images so you can actually see how the training progresses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the commented code below to _delete_ the folder\n",
    "# shutil.rmtree('./generated_images')\n",
    "\n",
    "# Use this line to make a subfolder to hold images\n",
    "os.makedirs('./generated_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.959899Z",
     "iopub.status.busy": "2021-06-16T15:52:14.959311Z",
     "iopub.status.idle": "2021-06-16T15:52:14.960870Z",
     "shell.execute_reply": "2021-06-16T15:52:14.961220Z"
    },
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    fig.suptitle(f\"Epoch {epoch}\")\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('./generated_images/image_at_epoch_{:03d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZrd4CdjR-Fp"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "The `train` function uses all of the aforementioned helpers to train the GAN over many iterations.\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real!\n",
    "\n",
    "The `epochs` parameter indicates how many iterations we will use. We will start by looping through all 1000 training images 50 times. \n",
    "\n",
    "This took about 145 seconds to run on my machine (who?). Hopefully it runs in similar time on yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.953573Z",
     "iopub.status.busy": "2021-06-16T15:52:14.952988Z",
     "iopub.status.idle": "2021-06-16T15:52:14.954759Z",
     "shell.execute_reply": "2021-06-16T15:52:14.955123Z"
    },
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    seed = tf.random.normal([16, INPUT_SIZE])\n",
    "    fullStart = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for the GIF as you go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,epoch + 1,seed)\n",
    "\n",
    "        print(f'Time for epoch {epoch + 1} is {time.time()-start} sec')\n",
    "        \n",
    "        global fakeError, realError\n",
    "        fakeError += [discriminator(generator(testInput), training=False).numpy()[0][0]]\n",
    "        realError += [discriminator(testImage, training=False).numpy()[0][0]]\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,epochs,seed)\n",
    "    print(f\"Total run time is {time.time()-fullStart}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:52:14.965100Z",
     "iopub.status.busy": "2021-06-16T15:52:14.964246Z",
     "iopub.status.idle": "2021-06-16T15:55:28.457250Z",
     "shell.execute_reply": "2021-06-16T15:55:28.457662Z"
    },
    "id": "Ly3UN0SLLY2l"
   },
   "outputs": [],
   "source": [
    "# THIS MAY TAKE A WHILE\n",
    "train(trainingData, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Training Analysis\n",
    "\n",
    "The GAN has been trained! We can start analyzing its performance by having the generator create an image and having the discriminator judge it as real or fake. We can also have the discriminator judge a real image to see if it can actually tell the difference between real and fake. Remember that we want the discriminator to NOT tell the difference. Ideally the discriminator will output positive numbers for both real _and_ fake images\n",
    "\n",
    "We can also view the data on the discriminator's judgement on the real and fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test real and fake images\n",
    "noise = tf.random.normal([1, INPUT_SIZE])            # random generator input\n",
    "indx = np.random.randint(NUM_SAMPLES)                # choose random real image to judge\n",
    "\n",
    "generated_image = generator(noise, training=False)   # generator output\n",
    "\n",
    "# DISCRIMINATOR JUDGEMENT\n",
    "real_image = cleanImageData[indx].reshape(1, 28, 28, 1)\n",
    "realImageDecision = discriminator(real_image)\n",
    "fakeImageDecision = discriminator(generated_image)\n",
    "print(\"Reminder that the discriminator gives positive values for real images and negative values for fake images\")\n",
    "print(f\"Fake image: {fakeImageDecision[0][0]}\")\n",
    "print(f\"Real image: {realImageDecision[0][0]}\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.imshow(generated_image[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "ax1.axis('off')\n",
    "ax1.title.set_text('Fake')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.imshow(real_image[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "ax2.axis('off')\n",
    "ax2.title.set_text('Real')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the data we collected\n",
    "epochTime = np.arange(1,100+1)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot(epochTime, realError)\n",
    "ax.plot(epochTime, fakeError)\n",
    "ax.legend([\"Real Image\", \"Fake Image\"])\n",
    "ax.set_title('Discriminator Judgement vs Time')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Judgement\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "# Images and GIFs\n",
    "\n",
    "For convenience, the `display_image` function below will access the image at the specified epoch from the folder you created.\n",
    "\n",
    "We can also create a GIF from these images for us to broadly judge how well the generator did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:55:28.533751Z",
     "iopub.status.busy": "2021-06-16T15:55:28.532979Z",
     "iopub.status.idle": "2021-06-16T15:55:28.545604Z",
     "shell.execute_reply": "2021-06-16T15:55:28.545939Z"
    },
    "id": "5x3q9_Oe5q0A"
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('./generated_images/image_at_epoch_{:03d}.png'.format(epoch_no))\n",
    "\n",
    "display_image(1)   # type a number here to see the image generated at that epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-16T15:55:28.550800Z",
     "iopub.status.busy": "2021-06-16T15:55:28.549994Z",
     "iopub.status.idle": "2021-06-16T15:55:29.032661Z",
     "shell.execute_reply": "2021-06-16T15:55:29.032063Z"
    },
    "id": "IGKQgENQ8lEI"
   },
   "outputs": [],
   "source": [
    "def createGIF():\n",
    "    anim_file = 'dcgan.gif'\n",
    "    with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "        filenames = glob.glob('./generated_images/image*.png')\n",
    "        filenames = sorted(filenames)\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    return anim_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.embed_file(createGIF())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variation of Parameters\n",
    "\n",
    "Hopefully you found that to be very interesting! However, the example GAN above definitely could be improved.\n",
    "\n",
    "Now it's your turn to train a GAN. Play around with the parameters in the cells below and re-train the model. Note that we have pasted the basic training function below because it needs to be freshly recompiled whenever you start training a new GAN. Be sure to run it too!\n",
    "\n",
    "Note that training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
    "\n",
    "Adjust the parameters in the cell below, then run the other cells below to redo the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL!!!\n",
    "EPOCHS = 50                 # ~50 epochs recommended\n",
    "noise_dim = 100             # between 20 and 300 recommended\n",
    "DIGIT = -1                  # -1 for all digits, otherwise 0-9 inclusive\n",
    "NUM_SAMPLES = 500           # -1 recommended if DIGIT == -1, otherwise <= 5000 recommended\n",
    "BUFFER_SIZE = NUM_SAMPLES   # NUM_SAMPLES recommended\n",
    "BATCH_SIZE = 20             # <= 10% of NUM_SAMPLES recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-COMPILE\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, INPUT_SIZE])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedImages = sortByNumber(DIGIT, NUM_SAMPLES)\n",
    "\n",
    "# create the 4D array!\n",
    "cleanImageData = sortedImages.reshape(sortedImages.shape[0], 28, 28, 1).astype('float32')   # 3D -> 4D\n",
    "cleanImageData = (cleanImageData - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "# data collection\n",
    "testImage = cleanImageData[np.random.randint(len(cleanImageData))].reshape(1, 28, 28, 1)\n",
    "testInput = tf.random.normal([1, INPUT_SIZE])\n",
    "fakeError = []\n",
    "realError = []\n",
    "\n",
    "# Batch and shuffle the data\n",
    "trainingData = tf.data.Dataset.from_tensor_slices(cleanImageData).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# WARNING - we are deleting the folder of images from the previous run\n",
    "shutil.rmtree('./generated_images')\n",
    "os.makedirs('./generated_images')\n",
    "\n",
    "if DIGIT != -1:\n",
    "    print(f\"There are {len(sortedImages)} images of the digit {DIGIT}\")\n",
    "else:\n",
    "    print(f\"There are {len(sortedImages)} images\")\n",
    "print(f\"There are {len(trainingData)} batches of {BATCH_SIZE} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(trainingData, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIF\n",
    "embed.embed_file(createGIF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test real and fake images\n",
    "noise = tf.random.normal([1, INPUT_SIZE])            # random generator input\n",
    "indx = np.random.randint(NUM_SAMPLES)                # choose random real image to judge\n",
    "\n",
    "generated_image = generator(noise, training=False)   # generator output\n",
    "\n",
    "# DISCRIMINATOR JUDGEMENT\n",
    "real_image = cleanImageData[indx].reshape(1, 28, 28, 1)\n",
    "realImageDecision = discriminator(real_image, training=False)\n",
    "fakeImageDecision = discriminator(generated_image, training=False)\n",
    "print(\"Reminder that the discriminator gives positive values for real images and negative values for fake images\")\n",
    "print(f\"Fake image: {fakeImageDecision[0][0]}\")\n",
    "print(f\"Real image: {realImageDecision[0][0]}\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.imshow(generated_image[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "ax1.axis('off')\n",
    "ax1.title.set_text('Fake')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.imshow(real_image[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "ax2.axis('off')\n",
    "ax2.title.set_text('Real')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the data we collected\n",
    "epochTime = np.arange(1,EPOCHS+1)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.plot(epochTime, realError)\n",
    "ax.plot(epochTime, fakeError)\n",
    "ax.legend([\"Real Image\", \"Fake Image\"])\n",
    "ax.set_title('Discriminator Judgement vs Time')\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Judgement\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjjkT9KAK6H7"
   },
   "source": [
    "This tutorial has shown the complete code necessary to write and train a GAN! To learn more about GANs see the [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160).\n",
    "\n",
    "If interested, feel free to explore some of the ideas below:\n",
    "\n",
    " - Experiment with a different dataset, for example the Large-scale Celeb Faces Attributes (CelebA) dataset [available on Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset). \n",
    "\n",
    " - Feel free to adjust parameters in the actual `make_generator_model` and `make_discriminator_model` functions. \n",
    "\n",
    "   - [dropout](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) in the discriminator CNN\n",
    "\n",
    "   - filter argument (see [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose))\n",
    "   - kernel argument (see [docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose))\n",
    "   \n",
    "Below are some links that I (who?) found useful \n",
    "\n",
    "\n",
    "[Tips On Training Your GANs Faster and Achieve Better Results](https://medium.com/intel-student-ambassadors/tips-on-training-your-gans-faster-and-achieve-better-results-9200354acaa5)\n",
    "\n",
    "[Understanding Input Output shapes in Convolution Neural Network | Keras](https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca)\n",
    "\n",
    "[Generative Adversarial Network (GAN) for Dummies — A Step By Step Tutorial](https://towardsdatascience.com/generative-adversarial-network-gan-for-dummies-a-step-by-step-tutorial-fdefff170391)\n",
    "\n",
    "[A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
